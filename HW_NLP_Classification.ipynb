{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daria-Lytvynenko/ML_course/blob/main/HW_NLP_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнє завдання: Побудова класифікатора сентименту на основі набору даних Tweet Sentiment Extraction\n",
        "\n",
        "**Мета:** Провести аналіз набору даних, виконати векторизацію текстових даних за допомогою методів bag-of-words та TF-IDF, порівняти їх, побудувати класифікатор та провести аналіз помилок.\n",
        "\n",
        "**Набір даних:**\n",
        "Дані беремо з цього змагання на Kaggle: https://www.kaggle.com/competitions/tweet-sentiment-extraction/data?select=train.csv\n",
        "\n",
        "Оригінальне змагання має дещо іншу задачу, але ми будемо поки будувати саме класифікатор.\n",
        "\n",
        "### Завдання 1. Завантаження та ознайомлення з набором даних\n",
        "\n",
        "- Завантажте набір даних `train.csv` з посилання та ознайомтеся з його структурою.\n",
        "- Виведіть перші 5 рядків та основну статистику: кількість записів, типи колонок, кількість пропущених значень.\n",
        "- Видаліть записи, в яких є пропущені значення.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zl1UHn4aRLMg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9PAZcZgddHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 2. Exploratory Data Analysis\n",
        "\n",
        "- Проведіть аналіз кількості класів та розподілу міток. Класи знаходяться в колонці `sentiment`.\n",
        "- Візуалізуйте розподіл довжин текстів в символах та зробіть висновок про довжини постів: якої довжини постів найбільше, що бачите з розподілу?\n",
        "\n"
      ],
      "metadata": {
        "id": "BXox7UCZUU5J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aPm_wCjpde0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 3. Попередня обробка текстових даних та векторизація з bag of words\n",
        "\n",
        "\n",
        "Наша задача тут отримати вектори методом bag of words колонки `text`, виконавши попередню обробку тексту.\n",
        "Попередня обробка має включати\n",
        "- видалення stopwords необхідної мови\n",
        "- токенізація (розбиття текстів на фрагменти по 1 слову)\n",
        "- стеммінг слів зі `SnowballStemmer`.\n",
        "- самостійно задайте кількість слів в словнику для `sklearn.feature_extraction.text.CountVectorizer`. Можливо для цього доведеться виконати додатковий аналіз.\n",
        "\n",
        "Ви також можете додати сюди додаткові методи очистки текстів, наприклад, видалення деяких символів чи груп символів, якщо в процесі роботи побачите, що хочете щось видалити.\n",
        "\n",
        "Напишіть код аби виконати це завдання. Перед цим рекомендую детально ознайомитись з тим, що робить обʼєкт `sklearn.feature_extraction.text.CountVectorizer` за замовченням.\n",
        "\n",
        "Це завдання можна виконати двома способами - один - максимально подібно до того, як ми це робили в лекції, другий - дещо інакше перегрупувавши етапи обробки тексту.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KfiU4hNDWncB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRUG111tdhB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 4. Побудова класифікатора\n",
        "\n",
        "- Розділіть індекси даних на навчальний та тестовий набори в обраному співвівдношенні. Використовуючи отримані індекси сфомуйте набори для тренування класифікатора `X_train_bow, X_test_bow, y_train, y_test`.\n",
        "- Навчіть класифікатор (наприклад, Logistic Regression, Decision Tree або один з алгоритмів бустингу) на даних, векторизованих методом bag-of-words. Спробуйте кілька моделей і оберіть найбільш точну :)\n",
        "- Виведіть інформацію, яка дає можливість оцінити якість класифікації.\n",
        "- Оцініть якість фінальної класифікації: вона хороша чи не дуже?\n",
        "\n"
      ],
      "metadata": {
        "id": "v0RHDwO7OBIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NVdpanFFdkyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 5. Аналіз впливовості слів в отриманого класифікатора\n",
        "\n",
        "- Для обраної вами моделі проведіть аналіз важливості слів (ознак): які слова (токени) найбільше впливають для визначення сентименту? Чи це логічно на ваш погляд, що саме ці символи впливають найбільше/найменще?\n"
      ],
      "metadata": {
        "id": "53hZa4bKP5Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0YBHvG4dmKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 6. Векторизація текстів з допомогою TF-IDF. Тренування класифікатора, аналіз точності і впливовості слів.\n",
        "\n",
        "- Проведіть векторизацію текстів з векторизатором TfidfVectorizer. Реалізуйте векторизацію так, аби препроцесинг включав всі ті самі кроки, що і в випадку використання векторизації Bag of Words.\n",
        "\n",
        "- Натренуйте той самий класифікатор на TF-IDF векторах, виконавши розбивку набору даних на train, test так, аби в трейні були всі ті самі записи, що і були в попередньому завданні (це важливо для порівняння результатів).\n",
        "\n",
        "- Проаналізуйте якість класифікації вивівши потрібні для цього метрики. Чи стала якість класифікації кращою?\n",
        "\n",
        "- Які токени найбільше впливають на результат при тренуваннні класифікатора з TF-IDF векторами? Порівняйте з найважливішими токенами при Bag of Words векторизації. Яку векторизацію ви б обрали для фінальної імплементації рішення? Обґрунтуйте свій вибір.\n",
        "\n"
      ],
      "metadata": {
        "id": "FvJlvr9JRhzq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "huUPpVhMdodx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 7. Аналіз помилок класифікації з векторизацією TF-IDF.\n",
        "\n",
        "- Проаналізуйте, на яких екземплярах помиляється класифікатор при векторизації TF-IDF.\n",
        "- На основі аналізу запропонуйте 3 шляхи поліпшення якості класифікації."
      ],
      "metadata": {
        "id": "3zsp9KftOqyS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbnvtiL8Y-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "І на фінал кернел для натхнення і ознайомлення з рішенням оригінальної задачі. Багато цікавих візуалізацій і аналізу є тут, а також тут розвʼязується саме проблема named entitty recognition і можна ознайомитись як це робиться - вона дещо складніша по своїй суті ніж класифікація, подумайте, чому:\n",
        "\n",
        "https://www.kaggle.com/code/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model"
      ],
      "metadata": {
        "id": "lFi4VWwjRS3h"
      }
    }
  ]
}