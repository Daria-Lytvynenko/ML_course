{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daria-Lytvynenko/ML_course/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=torch.from_numpy(inputs)\n",
        "targets=torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "5BlWoqk8QMqq",
        "outputId": "eb890b45-7d5b-4231-e0fa-c8739ee5648c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "MJ1PXTcuQOnQ",
        "outputId": "2283054c-9ead-4e20-b2c8-a03956f7bbdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "outputId": "ba5ecd9c-30f9-45c4-a256-48929c2ece02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c95c4576950>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.randn(1,3, requires_grad=True)\n",
        "b=torch.randn(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "id": "bpH14-u4RZlL",
        "outputId": "010a8efb-7e60-4083-fd7b-22a2d5400e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1974,  1.9428, -1.4017]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "fzRRAx9wRRQs",
        "outputId": "17ac02fa-3b7b-4124-ccbe-55972fecc599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7626], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKENjWgdGQ_",
        "outputId": "712e63c9-ee33-471e-9596-9c991ec177bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(inputs, w, b):\n",
        "  y_pred=1/(1+torch.exp(-(inputs@w.t()+b)))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "zBVZ1BQ1Vg9I"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs=model(inputs, w, b)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs"
      ],
      "metadata": {
        "id": "AyqUgoglaEGN",
        "outputId": "3a32c23e-3039-4752-e4b7-dd5a486c0030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4752],\n",
              "        [0.4669],\n",
              "        [0.4613],\n",
              "        [0.4761],\n",
              "        [0.4676]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(f\" pred_probs: {pred_probs.detach().numpy().flatten()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljUyllcldifE",
        "outputId": "5fd8b259-95b4-4bed-edc8-ad1e3cd81f01"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " pred_probs: [0.47518995 0.46692738 0.46133634 0.4761187  0.4675572 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(pred_probs, targets, epsilon=1e-7):\n",
        "  preds=torch.clamp(pred_probs, min=epsilon, max=1-epsilon)\n",
        "  loss=-torch.mean(targets*torch.log(preds)+(1-targets)*torch.log(1-preds))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=binary_cross_entropy(pred_probs, targets, epsilon=1e-7)"
      ],
      "metadata": {
        "id": "Ay1ZOz3MW0Vg"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQFqzSGTAP_0",
        "outputId": "6d1b5955-1302-41b7-af3f-3cec74ea3a8b"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.2612, 5.7466, 3.6881]])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHwRGHcmATwH",
        "outputId": "1e577472-5cd8-472d-f27a-32b92b76abef"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0858])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При збільшенні значень всіх трьох параметрів втрати зменшуються, оскільки значення градієнту від'ємні. Найбільше на значення функції втрат впливає другий параметр, оскільки він має найбільше абсолютне значення. Bias має невеликий вплив на функцію втрат."
      ],
      "metadata": {
        "id": "SW_Ms2hJM95G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(5)\n",
        "w = (torch.randn(1, 3) / 1000).requires_grad_(True)\n",
        "b = (torch.randn(1) / 1000).requires_grad_(True)"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d76e69d-43e4-4b54-fd06-51a1bbdf3981"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True),\n",
              " tensor([0.0006], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=binary_cross_entropy(pred_probs, targets)"
      ],
      "metadata": {
        "id": "_QOY0_AyViT3"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6ocEaRGEVl1m",
        "outputId": "9ef06079-e51e-45be-bcfb-954203464518"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-52a0569421b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(inputs, w, b, targets, learning_rate, epochs):\n",
        "    losses = []\n",
        "    for i in range(epochs):\n",
        "        pred_probs = model(inputs, w, b)\n",
        "        loss = binary_cross_entropy(pred_probs, targets, epsilon=1e-7)\n",
        "        loss.backward()\n",
        "        print(f\"Ітерація {i}:\")\n",
        "        print(f\"  Градієнт b: {b.grad}\")\n",
        "        print(f\"  Градієнт w: {w.grad}\")\n",
        "        print(f'w:{w}  b{b}')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            new_b = b - b.grad * learning_rate\n",
        "            new_w = w - w.grad * learning_rate\n",
        "            b.data.copy_(new_b.data)\n",
        "            w.data.copy_(new_w.data)\n",
        "            print(f'w:{w}  b{b}')\n",
        "\n",
        "        # Скидання градієнтів\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    return pred_probs, losses"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(inputs, w, b, targets, learning_rate=0.1, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8utPwHjQaHOy",
        "outputId": "3f2d698d-fc20-47dd-eefa-7ca0fd31c2aa"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ітерація 0:\n",
            "  Градієнт b: tensor([-0.1306])\n",
            "  Градієнт w: tensor([[ -9.7718, -23.5790, -12.9161]])\n",
            "w:tensor([[-0.0005, -0.0006, -0.0006]], requires_grad=True)  btensor([0.0007], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 1:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 2:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 3:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 4:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 5:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 6:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 7:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 8:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 9:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 10:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 11:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 12:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 13:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 14:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 15:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 16:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 17:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 18:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 19:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 20:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 21:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 22:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 23:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 24:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 25:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 26:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 27:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 28:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 29:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 30:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 31:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 32:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 33:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 34:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 35:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 36:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 37:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 38:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 39:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 40:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 41:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 42:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 43:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 44:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 45:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 46:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 47:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 48:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 49:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 50:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 51:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 52:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 53:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 54:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 55:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 56:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 57:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 58:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 59:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 60:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 61:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 62:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 63:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 64:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 65:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 66:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 67:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 68:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 69:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 70:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 71:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 72:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 73:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 74:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 75:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 76:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 77:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 78:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 79:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 80:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 81:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 82:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 83:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 84:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 85:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 86:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 87:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 88:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 89:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 90:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 91:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 92:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 93:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 94:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 95:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 96:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 97:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 98:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "Ітерація 99:\n",
            "  Градієнт b: tensor([0.])\n",
            "  Градієнт w: tensor([[0., 0., 0.]])\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n",
            "w:tensor([[0.9767, 2.3573, 1.2911]], requires_grad=True)  btensor([0.0137], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]], grad_fn=<MulBackward0>),\n",
              " [0.7173303365707397,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316,\n",
              "  6.376954078674316])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}