{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daria-Lytvynenko/ML_course/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=torch.from_numpy(inputs)\n",
        "targets=torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "5BlWoqk8QMqq",
        "outputId": "ac9f9381-670c-4287-8831-7804bb246c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "MJ1PXTcuQOnQ",
        "outputId": "403a0d0d-c902-40de-c7ba-40df07efa956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(2)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "outputId": "a3eced00-570a-4717-bdb3-e584216cb603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f595a2d9e10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.randn(1,3, requires_grad=True)\n",
        "b=torch.randn(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "id": "bpH14-u4RZlL",
        "outputId": "1f0306c8-0603-4220-9a6a-3d50808dbf2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3923, -0.2236, -0.3195]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "fzRRAx9wRRQs",
        "outputId": "7cf47a7b-b47b-41a8-f269-ef1c376ba038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.2050], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(inputs, w, b):\n",
        "  y_pred=1/(1+torch.exp(-(inputs@w.t()+b)))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "zBVZ1BQ1Vg9I"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs=model(inputs, w, b)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs"
      ],
      "metadata": {
        "id": "AyqUgoglaEGN",
        "outputId": "b2b14405-e338-4851-a979-08b7402e95b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1676e-01],\n",
              "        [3.5840e-03],\n",
              "        [1.7407e-07],\n",
              "        [1.0000e+00],\n",
              "        [1.5793e-08]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets*torch.log(pred_probs)"
      ],
      "metadata": {
        "id": "Yf2G63iMdS5l",
        "outputId": "aa3659d5-b96a-4954-db16-a9f42248c9be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.0000],\n",
              "        [ -5.6313],\n",
              "        [-15.5638],\n",
              "        [  0.0000],\n",
              "        [-17.9637]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1-targets)*torch.log(1-targets)"
      ],
      "metadata": {
        "id": "UrrrKZsqdYb4",
        "outputId": "a69d7aa9-4987-4507-b1c5-366274b7ac50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [0.],\n",
              "        [nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(pred_probs, targets, epsilon=1e-7):\n",
        "  preds=torch.clamp(pred_probs, min=epsilon, max=1-epsilon)\n",
        "  loss=-torch.mean(targets*torch.log(preds)+(1-targets)*torch.log(1-preds))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=binary_cross_entropy(pred_probs, targets)"
      ],
      "metadata": {
        "id": "Ay1ZOz3MW0Vg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "id": "AQFqzSGTAP_0",
        "outputId": "c03b0d5c-921d-47c5-e3dd-b2adac0ac256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-26.9951, -36.4990, -19.3236]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "id": "eHwRGHcmATwH",
        "outputId": "d9d7ccf8-66f9-4c82-ad11-b7bad7c5be56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.2823])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При збільшенні значень всіх трьох параметрів втрати зменшуються, оскільки значення градієнту від'ємні. Найбільше на значення функції втрат впливає другий параметр, оскільки він має найбільше абсолютне значення. Bias має невеликий вплив на функцію втрат."
      ],
      "metadata": {
        "id": "SW_Ms2hJM95G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs=model(inputs, w, b)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=binary_cross_entropy(pred_probs, targets)"
      ],
      "metadata": {
        "id": "_QOY0_AyViT3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "id": "NVLd3pPbVv9g",
        "outputId": "4b309e43-4561-4d5e-b09c-0d7bb2a8ffc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6829, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "6ocEaRGEVl1m"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "id": "mDEtzwNrV7R6",
        "outputId": "69133e56-4668-44f6-9ef4-46b71e66a4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -5.4417, -18.9853, -10.0682]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "id": "ZLjKBuRfV9Ra",
        "outputId": "bc7189ed-99f1-45de-ebcc-df664702504d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0794])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs"
      ],
      "metadata": {
        "id": "-GLTC7B0WC-2",
        "outputId": "42562564-6700-46eb-cfec-960fe0f0afa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5174],\n",
              "        [0.5220],\n",
              "        [0.5244],\n",
              "        [0.5204],\n",
              "        [0.5190]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b.data=b.data-b.data.grad*0.01"
      ],
      "metadata": {
        "id": "78KCvhDBa54M",
        "outputId": "605318f5-c4ed-4cb9-ee1a-5e26bbf2968a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-20cafa16780f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.data"
      ],
      "metadata": {
        "id": "0rbvxs6EbJW1",
        "outputId": "ebac433c-dd39-452f-f62d-c1bb6b2c5479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0006])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(inputs, w, b, targets, learning_rate, epochs):\n",
        "  losses=[]\n",
        "  for i in range(epochs):\n",
        "    pred_probs=model(inputs, w, b)\n",
        "    loss=binary_cross_entropy(pred_probs, targets)\n",
        "    loss.backward()\n",
        "    b_grad=b.grad\n",
        "    w_grad=w.grad\n",
        "    b.data-=b_grad*learning_rate\n",
        "    w.data-=w_grad*learning_rate\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    losses.append(loss)\n",
        "  return pred_probs, losses"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(inputs, w, b, targets, learning_rate=0.01, epochs=1000)"
      ],
      "metadata": {
        "id": "8utPwHjQaHOy",
        "outputId": "018dada8-aaa2-4cbf-ffb9-eb0b5ac2018e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]], grad_fn=<MulBackward0>),\n",
              " [tensor(0.6829, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>),\n",
              "  tensor(6.3770, grad_fn=<NegBackward0>)])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}