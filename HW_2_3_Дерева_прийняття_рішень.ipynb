{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daria-Lytvynenko/ML_course/blob/main/HW_2_3_%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%B0_%D0%BF%D1%80%D0%B8%D0%B9%D0%BD%D1%8F%D1%82%D1%82%D1%8F_%D1%80%D1%96%D1%88%D0%B5%D0%BD%D1%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В цьому домашньому завданні ми знову працюємо з даними з нашого змагання [\"Bank Customer Churn Prediction (DLU Course)\"](https://www.kaggle.com/t/7c080c5d8ec64364a93cf4e8f880b6a0).\n",
        "\n",
        "Тут ми побудуємо рішення задачі класифікації з використанням Decision Trees і зробимо новий submission на змагання на Kaggle.\n",
        "\n",
        "В цьому ДЗ ми працюємо без pipelines, бо так буде зручніше для візуалізації і інтерпретації моделі дерева прийняття рішень. Так буває і в робочих проєктах: іноді зручніше використати sklearn.Pipelines, іноді зручніше без них. На етапі пошуку рішення (research) зручніше без пайплайнів, а з пайплайнами - коли ви відлагодили процес обробки даних і хочете поекспериментувати з різними моделями і гіперпараметрами."
      ],
      "metadata": {
        "id": "8J0vzsqQfZlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання 1.**\n",
        "\n",
        "У попередньому домашньому завданні, `HW 2.7 Логістична регресія з scikit learn.ipynb`, ми писали обробку даних для змагання. Ваше завдання зараз - за прикладом, наведеним в лекції `Майстер-клас з перенесення коду з jupyter notebook у Python модуль`, перенести попередню обробку сирих даних з вашого розв'язку ДЗ 2.7 у файл `process_bank_churn.py` в функцію `preprocess_data(...)`.\n",
        "\n",
        "Функція `preprocess_data()` має приймати `raw_df` і вертати `X_train`, `train_targets`, `X_val`, `val_targets`, `input_cols`(перелік назв колонок, які Ви використовуєте в X), `scaler`, `encoder`, які ми потім будемо використовувати для тренування дерева прийняття рішень.\n",
        "\n",
        "\n",
        "### Кроки попередньої обробки:\n",
        "\n",
        "1. Обираємо колонки для роботи. В цьому завдання для чистоти експериментів рекомендую прибрати колонку `Surname`, так буде простіше інтрепретувати модель. Ви можете її додати вже за самостійних подальших експериментів.\n",
        "2. Розбиття сирих даних на тренувальні і валідаційні.\n",
        "3. Обробка категоріальних даних (one hot encoding).\n",
        "4. Масштабування числових даних (було частиною попередньої обробки в попередніх завданнях). Для дерев нам не обовʼязково масштабувати ознаки, тож в коді можна зробити цю частину опціональною, додавши в `preprocess_data` параметр `scaler_numeric`, який приймає значення `True` або `False`. Це дозволить використовувати код попередньоъ обробки з різними моделями.\n",
        "\n",
        "\n",
        "### Інструкції:\n",
        "1. Перенесіть попередню обробку даних у файл `process_bank_churn.py`.\n",
        "2. Забезпечте модулярну структуру функцій: кожна функція повинна виконувати лише одну дію, наприклад, масштабувати ознаки.\n",
        "3. Додайте докстрінги до кожної функції.\n",
        "4. Використовуйте typing для аргументів та значень, що повертаються функціями.\n",
        "5. Передбачте обробку нових даних. Додайте спеціальну функцію `preprocess_new_data(...)`, яка приймає на вхід нові дані в вигляді pandas DataFrame та використовує вже навчені скейлер та енкодер (передані теж як аргументи фукнціх) для їх обробки. Ця функція буде корисною для обробки нових даних перед передбаченням або оцінкою моделі, коли оброблятимемо `test.csv`.\n",
        "\n",
        "Можна і рекомендовано виконувати це завдання з ChatGPT (бажано, новіше, ніж 3.5) як було наведено у відео-прикладі, тільки **ваше додаткове завдання - розібратись з кодом, який вам згенерувала мовна модель :)**.\n",
        "\n",
        "## В результаті цього завдання\n",
        "\n",
        "1. Завантажте ваш готовий `process_bank_churn.py` файл на GitHub у свій репозиторій.\n",
        "2. Додайте посилання на файл в репозиторії тут у ноутбуці.\n",
        "3. Нижче зробіть імпорт функції `preprocess_data` з вашого модуля `process_bank_churn.py`.\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ2RvQ9G6H_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys"
      ],
      "metadata": {
        "id": "gJKP4iL38AEL",
        "outputId": "4ce7b24a-ff20-47f5-ae00-61ab03a00de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('drive/MyDrive/ML_course')"
      ],
      "metadata": {
        "id": "rzNHLazPj8Rc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls 'drive/MyDrive/ML_course'"
      ],
      "metadata": {
        "id": "5CtLo5elr7DD",
        "outputId": "8fffb1b2-7dae-4405-e346-dbbad6078402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "application_data.csv.zip\t medical-charges.csv\t       submission_log_reg.csv\n",
            "application_data_processed.zip\t model_pipeline.joblib\t       test.csv\n",
            "cars.csv\t\t\t previous_application.csv.zip  train.csv\n",
            "columns_description.csv\t\t process_bank_churn.py\t       train_inputs\n",
            "columns_description.gsheet\t __pycache__\t\t       train_targets\n",
            "customer_churn_pred.joblib\t regression_data.csv\t       val_inputs\n",
            "customer_segmentation_train.csv  sample_submission.csv\t       val_targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import process_bank_churn"
      ],
      "metadata": {
        "id": "LHLrruxEidEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(process_bank_churn))"
      ],
      "metadata": {
        "id": "Tarz3o2MrXbp",
        "outputId": "29992b5c-2850-4488-993d-8e9190f933ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'preprocess_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ML_course/process_bank_churn.py', 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "RyLwpjsrnz5X",
        "outputId": "21944ad9-7a68-4c5c-9b1b-d9763ce3ed83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def preprocess_data(raw_df):\n",
            "    raw_df.drop(['Surname', 'CustomerId'], axis=1, inplace=True)\n",
            "\n",
            "    X=raw_df.iloc[:,:-1]\n",
            "    y=raw_df.Exited\n",
            "    X_train, X_val, y_train, y_val=train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
            "    \n",
            "    input_cols=train.iloc[:, :-1].columns\n",
            "    target_col=train.iloc[:,-1].name\n",
            "    train_inputs=X_train\n",
            "    val_inputs=X_val\n",
            "    train_targets=y_train\n",
            "    val_targets=y_val\n",
            "    numeric_cols=train_inputs.select_dtypes(exclude='object').columns.tolist()\n",
            "    categorical_cols=train_inputs.select_dtypes(include='object').columns.tolist()\n",
            "\n",
            "    # Кодування категоріальних колонок\n",
            "    encoder=OneHotEncoder()\n",
            "    encoder.fit(train_inputs[categorical_cols])\n",
            "    geo_code_train=encoder.transform(train_inputs[categorical_cols]).toarray()\n",
            "    geo_code_val=encoder.transform(val_inputs[categorical_cols]).toarray()\n",
            "    categories=np.concatenate((encoder.categories_[0],encoder.categories_[1]))\n",
            "    train_inputs[categories]=geo_code_train\n",
            "    val_inputs[categories]=geo_code_val\n",
            "    encoder.categories_\n",
            "\n",
            "\n",
            "    # Масштабування числови колонок для приведення всіх значень до одного масштабу, оскільки різні колонки містять значення в різних діапазонах, що може негативно вплинути на\n",
            "    # сходження алгоритму\n",
            "    scaler=MinMaxScaler()\n",
            "    scaler.fit(train_inputs[numeric_cols])\n",
            "    train_inputs[numeric_cols]=scaler.transform(train_inputs[numeric_cols])\n",
            "    val_inputs[numeric_cols]=scaler.transform(val_inputs[numeric_cols])\n",
            "\n",
            "    cols=np.concatenate((numeric_cols, categories))\n",
            "    train_inputs=train_inputs[cols]\n",
            "    val_inputs=val_inputs[cols]\n",
            "    result={'X_train':train_inputs,\n",
            "    'train_targets':y_train,\n",
            "    'X_val':val_inputs,\n",
            "    'val_targets':y_val,\n",
            "    'input_cols':input_cols,\n",
            "    'scaler':scaler,\n",
            "    'encoder':encoder\n",
            "    }\n",
            "    return result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання 2.**\n",
        "1. Натренуйте дерево прийняття рішень з зафіксованим `random_state` і з рештою аргументів за замовченням.\n",
        "2. Виведіть area under ROC для моделі на тренувальних і тестувальних даних. Нам потрібна ця метрика, бо вона основна в змаганні. Модель ок, чи є пере- або недотренування?\n",
        "3. Виведіть глибину дерева.\n",
        "4. Побудуйте дерево до глибини 2 включно. Напишіть, які ознаки бачите, що є найвпливовішими тут?\n",
        "5. Створіть датафрейм `importance_df` з feature importances, де в першому стовпчику `feature` - назва ознаки з нашого Х, а в другому `importance` - значення, наскільки ця ознака є важливою в побудованій моделі. Виведіть топ 10 найвпливовіших ознак разом з їх скором важливості (можна або у вигляді таблиці, або в вигляді barplot)."
      ],
      "metadata": {
        "id": "B4YZMqOqaAH1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2U2dJIaF_W4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання 3**. Спробуйте знайти такі параметри моделі `DecisionTree` аби модель генералізувала ліпше (був вищий показник AUROC на валідаційній вибірці). Проекспериментуйте з різними значеннями параметрів `max_leaf_nodes`, `max_depth` та лишіть ті, які дають найкращий результат, разом з відповідними AUROC на тренувальній і валідаційній вибірках.\n",
        "\n",
        "В цьому завданні спробуйте просто знайти параметри методом \"тика\" :)"
      ],
      "metadata": {
        "id": "F-qp7eROd2MF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4PK69GAGCsc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання 4**. В циклі пройдіться по значенням max_depth від 1 до 20 включно, на кожній ітерації натренуйте DecisionTree модель і виміряйте AUROC на трейн і валідаційних даних.\n",
        "\n",
        "В кінці виведіть на графік залеєність між AUROC на трейн і валідаційних даних і номером ітерації."
      ],
      "metadata": {
        "id": "F-g1OXxofYqC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6RVg0TvGDu1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання 5**.\n",
        "1. Натренуйте модель `DecisionTree` з найкращим значенням `max_depth`, яке ви знайшли на попередній ітерації.\n",
        "2. Завантажте тестові дані змагання з `test.csv`.\n",
        "3. Зробіть попередню обробку даних з функцією `preprocess_new_data` з вашого модуля `process_bank_churn.py`.\n",
        "4. Зробіть передбачення використовуючи цю модель на тестових даних змагання.\n",
        "5. Сформуйте `submission.csv`.\n",
        "6. Зробіть новий Submission на Kaggle і додайте тут скріншот Вашого скору на паблік лідерборді :)\n"
      ],
      "metadata": {
        "id": "3cpULQtgiC5T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ht6fOjbQGH2D"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}